# Replication

[replica set]: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset

This section demonstrates how a [replica set] manages a
set of pods.

If you have a pod left over from previous lessons, get rid of it:
<!-- @deletePod -->
```
kubectl delete pod pod-tomato
```

Leave the load balancer service alone.  It cannot
successfully service requests because its only backing
service (the pod) has been deleted.

Confirm that it no longer works:

<!-- @hitServiceWithNewArgument -->
```
tut_Query bananna
```

Once the replica set starts making pods, the load
balancer will automatically hook up again by virtue of
the labels, demonstrating one of k8s's self-repair
behaviors.

Create a replica set:

<!-- @createReplicaSet -->
```yaml
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: repset-asparagus
spec:
  replicas: 3
  template:
    metadata:
      name: pod-tomato
      labels:
        app: avocado
        env: monkey-staging
    spec:
      containers:
      - name: cnt-carrot
        image: $TUT_IMG_TAG:$TUT_IMG_V1
        resources:
          limits:
            cpu: 100m
            memory: 10Mi
          requests:
            cpu: 100m
            memory: 10Mi
        ports:
        - name: port-pumpkin
          containerPort: 8080
EOF
```

<!-- @describeReplicaSet -->
```yaml
kubectl describe replicaset
```

There should now be three pods with names generated by
k8s:

<!-- @getPods -->
```
kubectl get pods
```

The service should take traffic now:
<!-- @hitServiceWithNewArgument -->
```
tut_Query jackfruit
```

## Examine the pods

Tailor a report to show the generated names for the
pods, along with their host IPs:

<!-- @getPodDetails -->
```
tmpl=`cat <<EOF
{{range .items -}}
{{.metadata.name -}}
 {{with .status -}}
    {{range .containerStatuses}}
      image: {{.image}}
       name: {{.name -}}
    {{end}}
     hostIp: {{.hostIP}}
        qos: {{.qosClass }}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" pods
```

Compare those IPs to the IPs of the nodes:

<!-- @detailTheNodes -->
```
tmpl=`cat <<EOF
{{range .items -}}
{{\\\$n := .metadata.name -}}
  {{range .status.addresses -}}
    {{if eq .type "InternalIP"}}{{\\\$n}} {{.address}}{{end -}}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" nodes
```

Confirm that each node (there's only one in the case of minikube)
has a copy of the image:

<!-- @grepNodesForProgram -->
```
kubectl get nodes -o yaml | grep $TUT_IMG_NAME
```

## Overpopulation

For fun, create a pod manually - with the app label
`avocado` - and watch (quickly) as it's immediately
terminated by the system.

<!-- @createOneTooMany -->
```
kubectl get pods
tut_CreatePod
kubectl get pods
```

## Underpopulation

Define function to delete a random pod:

<!-- @defineFunctionToDeleteRandomPod -->
```
function tut_DeleteRandomPod {
  local tmpl=`cat <<EOF
{{range .items -}}
{{.metadata.name}}
{{end}}
EOF
`
  local victim=$(kubectl get -o go-template="$tmpl" pods | sort -R | head -n 1)
  kubectl delete pod $victim
}
```

Define a pod watch that self-terminates in _n_ steps
(unlike `kubectl -w`):

<!-- @defineFunctionToWatchPods -->
```
function tut_WatchPods {
  kubectl get pods
  for i in $(seq 2 $1); do
    sleep 1
    kubectl get pods
  done
}
```

Delete a pod, and wait for it to be rescheduled:

<!-- @deleteRandomPod -->
```
kubectl get pods
tut_DeleteRandomPod
tut_WatchPods 20
```

If necessary, recreate the service, get
the (new) loadBalancer address, hit the LB, and try
interleaving requests with deletion of all pods.  See
if the pods stay dead long enough to fail a request.

<!-- @deleteAllPods -->
```
tut_Query peach
kubectl get pods
kubectl delete --all pods
tut_Query apple
tut_WatchPods 20
tut_Query peach
```

When done playing, delete the replica set:

<!-- @deleteReplicaSet -->
```
kubectl delete replicaset repset-asparagus
tut_WatchPods 20
```

The service can be left alone for now.

[Job]: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion
[Daemon Set]: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/

## Other pod managers

A pod intended to do some job and then die, e.g.
simply perform a git clone (using a git container), is
called a [Job].  The job manager will retry if the job
doesn't signal success.

A pod intended to run on each node outside the scope of
normal pod scheduling, e.g. to assure logs collection
on _every_ node, is part of a [Daemon Set].
