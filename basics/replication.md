# Replication

[ReplicaSet]: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset

Next try a [ReplicaSet].  Replica sets create pods, and
are supposed to be use in the context of a
_Deployment_, but we'll make a bare one here first.

Before doing so, delete the existing pod.

<!-- @deletePod -->
```
kubectl delete pod pod-tomato
```

Leave the load balancer service alone.  It cannot return
OK now because its only backing service (the pod) has been
deleted, but once the replica set starts making pods,
the load balancer will automatically hook up again by
virtue of the labels.

Create a replica set:

<!-- @createReplicaSet -->
```yaml
cat <<EOF | kubectl create -f -
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: repset-asparagus
spec:
  replicas: 2
  template:
    metadata:
      name: pod-tomato
      labels:
        app: avocado
        env: monkey-staging
    spec:
      containers:
      - name: cnt-carrot
        image: $TUT_IMG_TAG:$TUT_IMG_V1
        resources:
          limits:
            cpu: $TUT_CON_CPU
            memory: $TUT_CON_MEMORY
          requests:
            cpu: $TUT_CON_CPU
            memory: $TUT_CON_MEMORY
        ports:
        - name: port-pumpkin
          containerPort: 8080
EOF
```

<!-- @describeReplicaSet -->
```yaml
kubectl describe replicaset
```

There should now be three pods with names generated by
k8s:

<!-- @getPods -->
```
kubectl get pods
```

For fun, go back up and create a pod manually, and
watch (quickly) as it's immediately terminated
by the system.

Dump some useful pod info - like what nodes they live on:

<!-- @getPodDetails -->
```
tmpl=`cat <<EOF
{{range .items -}}
  name: {{.metadata.name -}}
  {{with .status -}}
    {{range .containerStatuses}}
      image: {{.image}}
       name: {{.name -}}
    {{end}}
     hostIp: {{.hostIP}}
        qos: {{.qosClass -}}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" pods
```

Compare those IPs to the IPs of the nodes:

<!-- @detailTheNodes -->
```
tmpl=`cat <<EOF
{{range .items -}}
{{\\\$n := .metadata.name -}}
  {{range .status.addresses -}}
    {{if eq .type "InternalIP"}}{{\\\$n}} {{.address}}{{end -}}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" nodes
```

Notice three instances of the image in the cluster
(one per node):

<!-- @grepNodesForProgram -->
```
kubectl get nodes -o yaml | grep $TUT_IMG_NAME
```

Define function to delete a random pod:

<!-- @defineFunctionToDeleteRandomPod -->
```
function tut_DeleteRandomPod {
  local tmpl=`cat <<EOF
{{range .items -}}
{{.metadata.name}}
{{end}}
EOF
`
  local victim=$(kubectl get -o go-template="$tmpl" pods | sort -R | head -n 1)
  kubectl delete pod $victim
}
```

Define a pod watch that self-terminates in _n_ steps
(unlike `kubectl -w`):

<!-- @defineFunctionToWatchPods -->
```
function tut_WatchPods {
  kubectl get pods
  for i in {1..6}; do
    sleep 0.3
    kubectl get pods
  done
}
```

Delete a pod, and wait for it to be rescheduled:

<!-- @deleteRandomPod -->
```
kubectl get pods
tut_DeleteRandomPod
tut_WatchPods
```

If necessary, recreate the service, get
the (new) loadBalancer address, hit the LB, and try
interleaving requests with deletion of all pods.  See
if the pods stay dead long enough to fail a request.

<!-- @deleteAllPods -->
```
tut_Query peach
kubectl get pods
kubectl delete --all pods
tut_Query apple
tut_WatchPods
tut_Query peach
```

When done, delete the replica set:

<!-- @deleteReplicaSet -->
```
kubectl delete replicaset repset-asparagus
tut_WatchPods
```

The service can be left alone for now.

[Job]: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion

Aside: A pod _intended_ to do some job and then die is called a [Job].
