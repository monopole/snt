# Self-Repairing Apps with Replication

> _Decimate the pods!  Reanimate the pods!_
>
> _Time: 9min_

[replica set]: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset

This section demonstrates how a [replica set] manages a
set of pods.

If you have pods running from earlier, get rid of it
to avoid confusing output below:
<!-- @deletePod -->
```
kubectl delete pods --all
```

The service can be left running, but it cannot serve
because its only backing service (the pod) has been
deleted.

<!-- @queryBusted -->
```
tut_Query bananna
```

Once the replica set starts making pods, the load
balancer will automatically hook up again by virtue of
the labels, demonstrating one of k8s's self-repair
behaviors.

Create a replica set:

<!-- @applyReplicaSet -->
```yaml
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: repset-asparagus
spec:
  replicas: 3
  template:
    metadata:
      name: pod-tomato
      labels:
        app: avocado
        env: monkey-staging
    spec:
      containers:
      - name: cnt-carrot
        image: $TUT_IMG_TAG:$TUT_IMG_V1
        resources:
          limits:
            cpu: 100m
            memory: 10Mi
          requests:
            cpu: 100m
            memory: 10Mi
        ports:
        - name: port-pumpkin
          containerPort: 8080
EOF
```

<!-- @descReplicaSet -->
```yaml
kubectl describe replicaset
```

There should now be three pods with names generated by
k8s:

<!-- @getPods -->
```
kubectl get pods
```

The service should take traffic now:
<!-- @queryService -->
```
tut_Query jackfruit
```

## Examine the pods

Tailor a report showing the generated pod names and
their host IPs:

<!-- @getPodDetails -->
```
tmpl=`cat <<EOF
{{range .items -}}
{{.metadata.name -}}
 {{with .status -}}
    {{range .containerStatuses}}
      image: {{.image}}
       name: {{.name -}}
    {{end}}
     hostIp: {{.hostIP}}
        qos: {{.qosClass }}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" pods
```

Compare those IPs to the IPs of the nodes:

<!-- @detailNodes -->
```
tmpl=`cat <<EOF
{{range .items -}}
{{\\\$n := .metadata.name -}}
  {{range .status.addresses -}}
    {{if eq .type "InternalIP"}}{{\\\$n}} {{.address}}{{end -}}
  {{end}}
{{end}}
EOF
`
kubectl get -o go-template="$tmpl" nodes
```

Confirm that each node (there's only one in the case of minikube)
has a copy of the image:

<!-- @grepNodesForImage -->
```
kubectl get nodes -o yaml | grep $TUT_IMG_NAME
```

## Overpopulation

Create a pod manually with the app label
`avocado`, and watch (quickly) as it's immediately
terminated by the system.

<!-- @oneTooMany -->
```
kubectl get pods
tut_CreatePod another-pod
kubectl get pods
```

```
kubectl get pods
```

## Underpopulation

Define function to delete a random pod:

<!-- @funcDeleteRandomPod -->
```
function tut_DeleteRandomPod {
  local tmpl=`cat <<EOF
{{range .items -}}
{{.metadata.name}}
{{end}}
EOF
`
  local victim=$(kubectl get -o go-template="$tmpl" pods | sort -R | head -n 1)
  kubectl delete pod $victim
}
```

Define a pod watch that self-terminates in _n_ steps
(unlike `kubectl -w`):

<!-- @funcToWatchPods -->
```
function tut_WatchPods {
  kubectl get pods
  for i in $(seq 2 $1); do
    sleep 1
    kubectl get pods
  done
}
```

Delete a pod, and wait for it to be rescheduled:

<!-- @deleteRandomPod -->
```
kubectl get pods
tut_DeleteRandomPod
tut_WatchPods 10
```

Delete _all_ pods, and try interleaving requests with
deletion of all pods.  See if the pods stay dead long
enough to fail a request.

<!-- @deleteAllPods -->
```
tut_Query peach
kubectl get pods
kubectl delete --all pods
tut_Query apple
tut_WatchPods 5
tut_Query peach
```

When done playing, delete the replica set:

<!-- @deleteReplicaSet -->
```
kubectl delete replicaset repset-asparagus
tut_WatchPods 3
```

The service can be left alone for now.

[Job]: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion
[Daemon Set]: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/

## Other pod managers

A pod intended to do some job and then die, e.g.
simply perform a git clone, is called a [Job].  The job
manager will retry if the job doesn't signal success.

A pod intended to run on each node outside the scope of
normal pod scheduling, e.g. to assure logs collection
on _every_ node, is part of a [Daemon Set].
